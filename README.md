# Panoptic-Depth-Estimation

## Setup

* [Install PyTorch](https://pytorch.org/get-started/locally/) compatible with your CUDA drivers, e.g for CUDA 11.1:
  `pip3 install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html`

* [Install Detectron2](https://detectron2.readthedocs.io/en/latest/tutorials/install.html) compatible with the PyTorch
  version:
  `python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html`

## Data

* Detectron2 Data Input/Output  https://detectron2.readthedocs.io/en/latest/tutorials/models.html

### NYUv2 Raw Depth 

* Folder structure
```
data/NYUv2-raw
├── official_splits
│   ├── nyu_depth_v2_labeled.mat 
│   ├── test
│   │   ├── bathroom
│   │   ├── bedroom
│   │   ├── ...
│   └── train
│       ├── basement
│       ├── bathroom
│       ├──...
├── raw
│   ├── sync
│   │   ├── basement_0001a
│   │   ├── basement_0001b
│   │   ├── bathroom_0001
│   │   ├── ...
│   │ 
│   └── sync.zip
├── splits.mat
└── train_test_inputs
    ├── nyudepthv2_test_files_with_gt.txt
    └── nyudepthv2_train_files_with_gt.tx
```

* Source: [BTS Paper](https://github.com/cleinc/bts/tree/9f026177dc82712a308438297391a76e786f46e2)
* Test set: Set of 654 test images (215 scenes) which also have the segmentation labels.
  * Setup: [Source](https://github.com/cleinc/bts#prepare-nyu-depth-v2-test-set) 
    * If you have already downloaded `nyu_depth_v2_labeled.mat`, create a link in the `data/NYUv2-raw/official_splits` folder.
    * Call `NYUv2LabeledDepthDatasetSetup("data/NYUv2-raw/official_splits")`.
  * Only the `test` subset of the labeled set will be used.
* Train set: 
  > From the total 120K image-depth pairs, due to asynchronous capturing rates between RGB images and depth maps,
  > we associate and sample them using timestamps by even-spacing in time, resulting in 24231 image-depth
  > pairs for the training set. Using raw depth images and camera projections provided by the dataset,
  > we align the image-depth pairs for accurate pixel registrations. 
  * Setup: [Source](https://github.com/cleinc/bts/blob/9f026177dc82712a308438297391a76e786f46e2/pytorch/README.md#preparation-for-training)
    ```shell
    python dataset/nyuv2_raw/download_from_gdrive.py 1AysroWpfISmm-yRFGBgFTrLy6FjQwvwP data/NYUv2-raw/raw/sync.zip
    cd data/NYUv2-raw/raw/
    unzip sync.zip
    ```
* Data points to be used for training/testing are listed under [`nyudepthv2_test_files_with_gt.txt`](data/NYUv2-raw/train_test_inputs/nyudepthv2_test_files_with_gt.txt) and
  [`nyudepthv2_train_files_with_gt.txt`](data/NYUv2-raw/train_test_inputs/nyudepthv2_train_files_with_gt.txt).
### COCO

* Download files
  using [coco_download.sh](coco_download.sh) ([Source](https://gist.github.com/mkocabas/a6177fc00315403d31572e17700d7fd9))
  .

* Set environment variable `DETECTRON2_DATASETS={workspaceDir}/data` so that Detectron can find the
  dataset ([Source](https://gist.github.com/mkocabas/a6177fc00315403d31572e17700d7fd9)).


* [Detectron requirements](https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html):

  #### Expected dataset structure for PanopticFPN:

  Extract panoptic annotations from [COCO website](https://cocodataset.org/#download)
  into the following structure:
    ```
    coco/
      annotations/
        panoptic_{train,val}2017.json
      panoptic_{train,val}2017/  # png annotations
      panoptic_stuff_{train,val}2017/  # generated by the script mentioned below
    ```

  Install panopticapi by:
    ```
    pip install git+https://github.com/cocodataset/panopticapi.git
    ```
  Then, run `python datasets/prepare_panoptic_fpn.py`, to extract semantic annotations from panoptic annotations.
  > See [`dataset/coco/prepare_panoptic_fpn.py`](`dataset/coco/prepare_panoptic_fpn.py`).
* [Official COCO Website](https://cocodataset.org/#download)
